{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import numpy as np\n",
    "\n",
    "import io\n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import pairwise\n",
    "\n",
    "import os\n",
    "import tensorboard\n",
    "from tensorboard.plugins import projector\n",
    "\n",
    "import collections\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = tfds.load('multi_news', split='train', with_info=True)\n",
    "\n",
    "# #use index to get specific document\n",
    "# index = 20\n",
    "# count = 0\n",
    "# with tf.Graph().as_default():\n",
    "#     numpy_imgs = next(iter(ds))\n",
    "#     # numpy_imgs = tfds.as_numpy(ds)\n",
    "# count = 0\n",
    "# document = []\n",
    "# summary = []\n",
    "# for x in numpy_imgs:\n",
    "#     count += 1\n",
    "#     if count == index:\n",
    "#         # tf.print(x[\"document\"])\n",
    "#         # print(\"\\n\")\n",
    "#         # print(\"\\n\")\n",
    "#         # print(\"SUMMARY\")\n",
    "#         # tf.print(x[\"summary\"])\n",
    "\n",
    "#         document = x[\"document\"]\n",
    "#         summary = x[\"summary\"]\n",
    "#         break\n",
    "# document = bytes(document.numpy())\n",
    "# document = [document.decode(\"utf-8\")]\n",
    "\n",
    "# summary = bytes(summary.numpy())\n",
    "# summary = [summary.decode(\"utf-8\")]\n",
    "# #create vocab\n",
    "# d_tokens = document[0].lower().split()\n",
    "# s_tokens = summary[0].lower().split()\n",
    "# tokens = d_tokens + s_tokens\n",
    "# vocab, index = {}, 1\n",
    "# vocab[\"<pad>\"] = 0\n",
    "# for token in tokens:\n",
    "#     if token not in vocab:\n",
    "#         vocab[token] = index\n",
    "#         index = index + 1\n",
    "\n",
    "# inverse_vocab = {index: token for token, index in vocab.items()}\n",
    "# example = [vocab[word] for word in s_tokens]\n",
    "# embed = hub.load(\"https://tfhub.dev/google/nnlm-en-dim128/2\")\n",
    "# embeddings = embed([\"cat is on the mat\", \"dog is in the fog\"])\n",
    "# print(embeddings)\n",
    "# # stuff to do\n",
    "# # get vocabulary\n",
    "# # display output in a visual way\n",
    "# #lean word2vec\n",
    "\n",
    "# preprocess = hub.load('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')\n",
    "# bert = hub.load('https://tfhub.dev/google/experts/bert/wiki_books/2')\n",
    "\n",
    "# sentences = [\n",
    "#   \"Here We Go Then, You And I is a 1999 album by Norwegian pop artist Morten Abel. It was Abel's second CD as a solo artist.\",\n",
    "#   \"The album went straight to number one on the Norwegian album chart, and sold to double platinum.\",\n",
    "#   \"Among the singles released from the album were the songs \\\"Be My Lover\\\" and \\\"Hard To Stay Awake\\\".\",\n",
    "#   \"Riccardo Zegna is an Italian jazz musician.\",\n",
    "#   \"Rajko Maksimović is a composer, writer, and music pedagogue.\",\n",
    "#   \"One of the most significant Serbian composers of our time, Maksimović has been and remains active in creating works for different ensembles.\",\n",
    "#   \"Ceylon spinach is a common name for several plants and may refer to: Basella alba Talinum fruticosum\",\n",
    "#   \"A solar eclipse occurs when the Moon passes between Earth and the Sun, thereby totally or partly obscuring the image of the Sun for a viewer on Earth.\",\n",
    "#   \"A partial solar eclipse occurs in the polar regions of the Earth when the center of the Moon's shadow misses the Earth.\",\n",
    "# ]\n",
    "\n",
    "# wordArray = []\n",
    "\n",
    "# for i in sentences:\n",
    "#   words = i.split()\n",
    "#   for w in words:\n",
    "#     w = w.replace(\",\", \"\")\n",
    "#     w = w.replace(\".\", \"\")\n",
    "#     wordArray.append(w)\n",
    "\n",
    "# print(\"word array\")\n",
    "# print(wordArray)\n",
    "\n",
    "# bert_inputs = preprocess(sentences)\n",
    "# bert_outputs = bert(bert_inputs)\n",
    "# pooled_output = bert_outputs['pooled_output']\n",
    "# sequence_output = bert_outputs['sequence_output']\n",
    "\n",
    "# print('\\nPooled output:')\n",
    "# print(pooled_output)\n",
    "# print('\\nSequence output:')\n",
    "# print(sequence_output)\n",
    "\n",
    "# def plot_similarity(features, labels):\n",
    "#   \"\"\"Plot a similarity matrix of the embeddings.\"\"\"\n",
    "#   cos_sim = pairwise.cosine_similarity(features)\n",
    "#   sns.set(font_scale=1.2)\n",
    "#   cbar_kws=dict(use_gridspec=False, location=\"left\")\n",
    "#   g = sns.heatmap(\n",
    "#       cos_sim, xticklabels=labels, yticklabels=labels,\n",
    "#       vmin=0, vmax=1, cmap=\"Blues\", cbar_kws=cbar_kws)\n",
    "#   g.tick_params(labelright=True, labelleft=False)\n",
    "#   g.set_yticklabels(labels, rotation=0)\n",
    "#   g.set_title(\"Semantic Textual Similarity\")\n",
    "\n",
    "# plot_similarity(bert_outputs[\"pooled_output\"], wordArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
    "# (int-encoded sentences) based on window size, number of negative samples\n",
    "# and vocabulary size.\n",
    "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
    "  # Elements of each training example are appended to these lists.\n",
    "  targets, contexts, labels = [], [], []\n",
    "\n",
    "  # Build the sampling table for vocab_size tokens.\n",
    "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "  # Iterate over all sequences (sentences) in dataset.\n",
    "  for sequence in tqdm.tqdm(sequences):\n",
    "\n",
    "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
    "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "          sequence,\n",
    "          vocabulary_size=vocab_size,\n",
    "          sampling_table=sampling_table,\n",
    "          window_size=window_size,\n",
    "          negative_samples=0)\n",
    "\n",
    "    # Iterate over each positive skip-gram pair to produce training examples\n",
    "    # with positive context word and negative samples.\n",
    "    for target_word, context_word in positive_skip_grams:\n",
    "      context_class = tf.expand_dims(\n",
    "          tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "          true_classes=context_class,\n",
    "          num_true=1,\n",
    "          num_sampled=num_ns,\n",
    "          unique=True,\n",
    "          range_max=vocab_size,\n",
    "          seed=SEED,\n",
    "          name=\"negative_sampling\")\n",
    "\n",
    "      # Build context and label vectors (for one target word)\n",
    "      negative_sampling_candidates = tf.expand_dims(\n",
    "          negative_sampling_candidates, 1)\n",
    "\n",
    "      context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
    "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "      # Append each element from the training example to global lists.\n",
    "      targets.append(target_word)\n",
    "      contexts.append(context)\n",
    "      labels.append(label)\n",
    "\n",
    "  return targets, contexts, labels\n",
    "\n",
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  return tf.strings.regex_replace(lowercase,\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')\n",
    "\n",
    "class Word2Vec(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim):\n",
    "    super(Word2Vec, self).__init__()\n",
    "    self.target_embedding = layers.Embedding(vocab_size,\n",
    "                                      embedding_dim,\n",
    "                                      input_length=1,\n",
    "                                      name=\"w2v_embedding\")\n",
    "    self.context_embedding = layers.Embedding(vocab_size,\n",
    "                                       embedding_dim,\n",
    "                                       input_length=num_ns+1,\n",
    "                                       name=\"w2v_context\")\n",
    "\n",
    "  def call(self, pair):\n",
    "    target, context = pair\n",
    "    # target: (batch, dummy?)  # The dummy axis doesn't exist in TF2.7+\n",
    "    # context: (batch, context)\n",
    "    if len(target.shape) == 2:\n",
    "      target = tf.squeeze(target, axis=1)\n",
    "    # target: (batch,)\n",
    "    word_emb = self.target_embedding(target)\n",
    "    # word_emb: (batch, embed)\n",
    "    context_emb = self.context_embedding(context)\n",
    "    # context_emb: (batch, context, embed)\n",
    "    dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
    "    # dots: (batch, context)\n",
    "    return dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "num_ns = 4\n",
    "\n",
    "ngram_freq = pd.read_csv('ngram_freq.csv')\n",
    "ngramWordList = list(ngram_freq['word'].values)\n",
    "\n",
    "def textToArray(file_path):\n",
    "    # turn text into array of words\n",
    "    text_ds = tf.data.TextLineDataset(file_path).filter(lambda x: tf.cast(tf.strings.length(x), bool))\n",
    "    text_ds = text_ds.enumerate()\n",
    "    docTxt = []\n",
    "    for i in text_ds.as_numpy_iterator():\n",
    "        line = i[1].decode().split()\n",
    "        for l in line:\n",
    "            docTxt.append(l)\n",
    "    \n",
    "    return docTxt\n",
    "\n",
    "def getVocab(file_path):\n",
    "    doc_ds = tf.data.TextLineDataset(file_path).filter(lambda x: tf.cast(tf.strings.length(x), bool))\n",
    "    vocab_size = 4096\n",
    "    sequence_length = 220\n",
    "    vectorize_layer = layers.TextVectorization(\n",
    "\n",
    "        standardize=custom_standardization,\n",
    "        max_tokens=vocab_size,\n",
    "        output_mode='int',\n",
    "        output_sequence_length=sequence_length)\n",
    "\n",
    "    vectorize_layer.adapt(doc_ds.batch(1024))\n",
    "\n",
    "    doc_vector_ds = doc_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()\n",
    "    sequences = list(doc_vector_ds.as_numpy_iterator())\n",
    "\n",
    "    doc_vocab = vectorize_layer.get_vocabulary()\n",
    "    return doc_vocab, sequences\n",
    "\n",
    "def findKeyWords(docTxt, vocab, ngramWordList):\n",
    "    #find the key words in the document\n",
    "    keyWords = {}\n",
    "    docFeq = collections.Counter(docTxt)\n",
    "    maxWordFreq = max(docFeq, key=docFeq.get)\n",
    "    maxFreq = docFeq[str(maxWordFreq)]\n",
    "\n",
    "    for v in vocab:\n",
    "        try:\n",
    "            realFreq = docFeq[str(v)]\n",
    "            v_index = ngramWordList.index(str(v)) + 1\n",
    "            nFreq = int(maxFreq / v_index)\n",
    "\n",
    "            if realFreq > nFreq and v_index > 20 and realFreq > 0:\n",
    "                keyWords[str(v)] = str(realFreq) + \" | \" + str(nFreq)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return keyWords\n",
    "\n",
    "def generateTupleTraining(file_path, doc_vocab, keyWords, sequences):\n",
    "    vec_keyWords = []\n",
    "    for k in keyWords:\n",
    "        vecNum = doc_vocab.index(str(k))\n",
    "        vec_keyWords.append(vecNum)\n",
    "\n",
    "    #labels: 1 - positive sample, 0 - negative sample\n",
    "    targets, contexts, labels = generate_training_data(\n",
    "        sequences=sequences,\n",
    "        window_size=2,\n",
    "        num_ns=4,\n",
    "        vocab_size=(len(doc_vocab) + 1),\n",
    "        seed=SEED)\n",
    "\n",
    "    keySamples = []\n",
    "    for index, t in enumerate(targets):\n",
    "        if t in vec_keyWords:\n",
    "            for c_index, c in enumerate(contexts[index]):\n",
    "                sample = [t, int(c[0]), int(labels[index][c_index])]\n",
    "                keySamples.append(sample)\n",
    "    \n",
    "    return keySamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89/89 [00:00<00:00, 513.81it/s]\n"
     ]
    }
   ],
   "source": [
    "file_path = 'document.txt'\n",
    "docTxt = textToArray(file_path)\n",
    "doc_vocab, sequences = getVocab(file_path)\n",
    "keyWords = findKeyWords(docTxt, doc_vocab, ngramWordList)\n",
    "keySamples = generateTupleTraining(file_path, doc_vocab, keyWords, sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createWordVectors(file_path, useBoard):\n",
    "    text_ds = tf.data.TextLineDataset(file_path).filter(lambda x: tf.cast(tf.strings.length(x), bool))\n",
    "    \n",
    "    # Define the vocabulary size and number of words in a sequence.\n",
    "    vocab_size = 4096\n",
    "    sequence_length = 300\n",
    "\n",
    "    # Use the TextVectorization layer to normalize, split, and map strings to\n",
    "    # integers. Set output_sequence_length length to pad all samples to same length.\n",
    "    vectorize_layer = layers.TextVectorization(\n",
    "        standardize=custom_standardization,\n",
    "        max_tokens=vocab_size,\n",
    "        output_mode='int',\n",
    "        output_sequence_length=sequence_length)\n",
    "\n",
    "    vectorize_layer.adapt(text_ds.batch(1024))\n",
    "\n",
    "    inverse_vocab = vectorize_layer.get_vocabulary()\n",
    "\n",
    "    text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()\n",
    "\n",
    "    sequences = list(text_vector_ds.as_numpy_iterator())\n",
    "    \n",
    "    targets, contexts, labels = generate_training_data(\n",
    "        sequences=sequences,\n",
    "        window_size=2,\n",
    "        num_ns=4,\n",
    "        vocab_size=vocab_size,\n",
    "        seed=SEED)\n",
    "\n",
    "    contexts = np.array(contexts)[:,:,0]\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # BATCH_SIZE = 1024\n",
    "    # BUFFER_SIZE = 10000\n",
    "    BATCH_SIZE = 10\n",
    "    BUFFER_SIZE = 15\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "    dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "    dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    embedding_dim = 128\n",
    "    word2vec = Word2Vec(vocab_size, embedding_dim)\n",
    "    word2vec.compile(optimizer='adam',\n",
    "                    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                    metrics=['accuracy'])\n",
    "    \n",
    "    if useBoard:\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
    "        word2vec.fit(dataset, epochs=1, callbacks=[tensorboard_callback])\n",
    "    else:\n",
    "        word2vec.fit(dataset, epochs=5)\n",
    "\n",
    "    tWeights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
    "    cWeights = word2vec.get_layer('w2v_context').get_weights()[0]\n",
    "    vocab = vectorize_layer.get_vocabulary()\n",
    "\n",
    "    # tf_weight = tf.Variable(tWeights)\n",
    "    # checkpoint = tf.train.Checkpoint(embedding=tf_weight)\n",
    "    # checkpoint.save(os.path.join('logs', \"embedding.ckpt\"))\n",
    "    return tWeights, cWeights, vocab, sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 200.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2/2 [==============================] - 1s 9ms/step - loss: 1.6086 - accuracy: 0.2000\n",
      "Epoch 2/5\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 1.5992 - accuracy: 0.6500\n",
      "Epoch 3/5\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.5904 - accuracy: 0.9000\n",
      "Epoch 4/5\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.5816 - accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 1.5726 - accuracy: 1.0000\n",
      "Average accuracy: 0.5\n",
      "[0.49886876]\n",
      "[0.00024224193514665274, 0.0002480595562089633, 0.0002437059217578762, 0.0002430118644912268, 0.00024826462527465355, 0.00024532397218979696, 0.00023970731402737133, 0.00024361723062414414, 0.0002453537729743801, 0.00023795548597495584, 0.00024524780257148413, 0.0002453537729743801, 0.00024353767112612044, 0.00023738330502646288, 0.0002453537729743801, 0.00023738330502646288, 0.00024384973825599405, 0.00024567176352856414, 0.00024282790488327043, 0.000246801715058369, 0.000242601200886798, 0.0002423017864610616, 0.0002468082160483504, 0.0002448264904533975, 0.0002468082160483504, 0.0002445092301159489, 0.00024313790091847582, 0.0002500921448241859, 0.0002420284309475297, 0.00024391193622293905, 0.0002437131486210327, 0.0002460737457602085, 0.0002437131486210327, 0.0002420284309475297, 0.00024524443993629154, 0.0002448599619376855, 0.0002487425996883957, 0.0002433645207950941, 0.0002433645207950941, 0.0002487425996883957, 0.00024524443993629154, 0.0002480238810691833, 0.00024602985697398726, 0.00024364149361035575, 0.0002443662443700438, 0.00024651949014272795, 0.000247278282571287, 0.00024602985697398726, 0.00024602985697398726, 0.0002448825125404284, 0.00024161932228898615, 0.00024274226246335127, 0.0002429924927262086, 0.00024274226246335127, 0.0002458576933394787, 0.0002434274424921465, 0.00024351923008680826, 0.000248902847414465, 0.00024274226246335127, 0.00024457119116086477, 0.0002430133029521205, 0.00024624456875304944, 0.0002446650146446721, 0.00024383812487829913, 0.00024354520490477814, 0.00024146273398581957, 0.0002443300616057039, 0.00024354520490477814, 0.0002497199788933532, 0.00024354520490477814, 0.00024146273398581957, 0.00024399600314094257, 0.00024633711311351677, 0.0002429079458288199, 0.00024398116687664168, 0.00024633711311351677, 0.00024757164275069775, 0.00024382502262296468, 0.00024398116687664168, 0.00024397626664772576, 0.00024240414448385957, 0.0002487425996883957, 0.00024745302666653965, 0.0002452138067886573, 0.0002490481520827708, 0.00024379239342050977, 0.0002424332079238282, 0.00024292476465049165, 0.00024206017537065216, 0.00024292476465049165, 0.00024311992013977367, 0.00024292476465049165, 0.0002424332079238282, 0.00024352094911342964, 0.00023862997449582875, 0.0002445947655997422, 0.0002441714718239413, 0.00024403627177986274, 0.0002483696860542224, 0.00024422094169226845, 0.00024390378945509877, 0.0002419964252659613, 0.0002483696860542224, 0.0002445368402706265, 0.00024390027096414898, 0.00024390027096414898, 0.0002448599619376855, 0.00024249478898460627, 0.00024397626664772576, 0.00024548492073807093, 0.0002460415405184804, 0.00024124481286692557, 0.00024231601808551238, 0.00024398502480465855, 0.00024398502480465855, 0.00024624456875304944, 0.00024548492073807093, 0.0002415072192864792, 0.00024508287786734873, 0.0002460415405184804, 0.00024336807337961965, 0.00023862997449582875, 0.00024682639411734565, 0.00024682639411734565, 0.0002426194454291872, 0.00024275836138545125, 0.00024682639411734565, 0.00024330399990531873, 0.00024721069064274, 0.00024278882833145862, 0.00024570152302161, 0.0002408642838703005, 0.00024540134200811125, 0.00024278882833145862, 0.0002438196628658085, 0.00024278882833145862, 0.0002449109494313738, 0.00024570152302161, 0.00024293805133233795, 0.00024617486240207075, 0.00024647867028866735, 0.00024641118345710274, 0.00024811001729705014, 0.00024330200664365935, 0.00024525824495050475, 0.00024524443993629154, 0.0002448599619376855, 0.0002442147942070218, 0.0002487425996883957, 0.0002448599619376855, 0.0002487425996883957, 0.0002490481520827708, 0.00024776719118831174, 0.00024314132478057076, 0.0002437658656895555, 0.00024431076586387224, 0.0002427781234295319, 0.0002427369676015287, 0.00024746045070898117, 0.00024145802149591294, 0.0002439338652595026, 0.000245915139356861, 0.000244324146361668, 0.000245915139356861, 0.000244324146361668, 0.00023795548597495584, 0.00024199032551857347, 0.00024524780257148413, 0.00024904770463074396, 0.00024524780257148413, 0.00023795548597495584, 0.00024841909857815426, 0.00024506333687807856, 0.0002455573016720925, 0.00024384973825599405, 0.00024396962659924715, 0.00024713426655860823, 0.00024248827033064525, 0.00025062940334567125, 0.00024527079632279933, 0.00024287632473919064, 0.00024223346526395465, 0.00024833878223023595, 0.00024481135593647577, 0.00023862997449582875, 0.00024303646679077993, 0.00024157764850224117, 0.00024403627177986274, 0.0002425999594309136, 0.00024620214689304346, 0.00024403627177986274, 0.00024833878223023595, 0.00024336807337961965, 0.0002453947742409374, 0.000244396565346484, 0.0002446143214886835, 0.00024729965832977755, 0.0002480760145147608, 0.0002427781234295319, 0.00024145802149591294, 0.0002453947742409374, 0.00024729965832977755, 0.0002437658656895555, 0.0002480760145147608, 0.00024547292724564956, 0.00024688465054687175, 0.0002444303622246704, 0.0002441334951569486, 0.00024391991485947907, 0.0002441334951569486, 0.0002441334951569486, 0.00024688465054687175, 0.00024016525679045362, 0.00024391991485947907, 0.00024467286174290276, 0.0002487058244007445, 0.00024549907528359223, 0.00024380004984840483, 0.00024290235453501395, 0.00024549907528359223, 0.00024569876123660476, 0.00024346630606077348, 0.00024408787274714504, 0.00024461705452860535, 0.00024382502262296468, 0.00024344533361347358, 0.0002503641101471956, 0.00024533241686641044, 0.00024398116687664168, 0.00024382502262296468, 0.0002487425996883957, 0.00024146709656987084, 0.00024776719118831174, 0.00024524443993629154, 0.0002427781234295319, 0.0002453947742409374, 0.0002437658656895555, 0.0002437658656895555, 0.0002453947742409374, 0.0002427781234295319, 0.0002427781234295319, 0.0002466767886065575, 0.0002460534656220937, 0.00024729965832977755, 0.00024648684396220033, 0.0002454301609882874, 0.000245915139356861, 0.0002419633144498015, 0.0002439338652595026, 0.0002487058244007445, 0.00024547292724564956, 0.000244324146361668, 0.0002419633144498015, 0.00024369420729848258, 0.000243945411757381, 0.0002419633144498015, 0.00024671671160058105, 0.00024157764850224117, 0.0002441714718239413, 0.00024303646679077993, 0.00024450685990328076, 0.00024545446817177813, 0.0002445947655997422, 0.0002441714718239413, 0.00024336807337961965, 0.00024688465054687175, 0.0002423916725896205, 0.00024821324223730774, 0.00024547292724564956, 0.0002451539157470048, 0.00024688465054687175, 0.00024210827622515475, 0.00024798567559953524, 0.00024431616425617866, 0.0002441813883828801, 0.00024107315018826597, 0.0002439338652595026, 0.0002444943783750396, 0.0002419633144498015, 0.0002439338652595026, 0.0002419633144498015, 0.0002439338652595026, 0.000244324146361668, 0.000245915139356861, 0.000244324146361668, 0.0002455682513203415, 0.0002438936775813584, 0.0002430324150464162, 0.00024325004212793382, 0.00024325004212793382, 0.0002419459068594555, 0.000246801715058369, 0.0002448264904533975, 0.000246801715058369, 0.00024567176352856414, 0.00024567176352856414, 0.000242601200886798, 0.0002482634764378962, 0.00024293948101405553, 0.0002408483528255749, 0.0002420284309475297, 0.00024455717077813844, 0.0002394526647287167, 0.0002420284309475297, 0.00024313790091847582, 0.000241464149151182, 0.00024371824222232973, 0.00024745747582719836, 0.00024690711435465225, 0.00024392202519542714, 0.00024690711435465225, 0.0002479003633591744, 0.00024303000666700618, 0.00024415431050630703, 0.0002448264904533975, 0.00023960765086204368, 0.0002419459068594555, 0.000242601200886798, 0.00024293948101405553, 0.00024487535126835547, 0.00024487535126835547, 0.00024399600314094257, 0.00024399600314094257, 0.00024241706454031264, 0.00024314238511556605, 0.0002448336147496031, 0.00024354520490477814, 0.0002497199788933532, 0.0002419964252659613, 0.0002457986783721674, 0.00024414225923168086, 0.0002445024515514826, 0.00024731300644225735, 0.0002457986783721674, 0.00024422094169226845, 0.0002483696860542224, 0.00024223744676907225, 0.00024258700802788212, 0.0002429871339128543, 0.00024390930882781995, 0.0002438091347009698, 0.00024593386414542394, 0.0002403963325907603, 0.00024329791840984184, 0.0002433366808786119, 0.00024545446817177813, 0.00023862997449582875, 0.00024421031038944344, 0.00024421031038944344, 0.00024450685990328076, 0.00024258627220991486, 0.0002459216927423711, 0.0002444560677876998, 0.0002453537729743801, 0.00024904770463074396, 0.0002459216927423711, 0.0002459216927423711, 0.0002453537729743801, 0.0002444560677876998, 0.00024548492073807093, 0.00024628394931669895, 0.00024624456875304944, 0.00024508287786734873, 0.00024487900090740386, 0.00024288603606826135, 0.0002443002435119297, 0.00024359055768995446, 0.0002456617027150502, 0.00024171313199060517, 0.00024359055768995446, 0.00024171313199060517, 0.00024292525733083127, 0.00024359055768995446, 0.0002443002435119297, 0.00024604602764528296, 0.00024604602764528296, 0.00024062598372281277, 0.00024171313199060517, 0.00024139860774600517, 0.00024139860774600517, 0.000244498185927722, 0.00024023361157612686, 0.00024250734587855303, 0.00024373478983000192, 0.00024234762389250338, 0.00024303646679077993, 0.0002435830713025728, 0.00024545446817177813, 0.0002423356589516331, 0.00024271616004082914, 0.00024604602764528296, 0.00024062598372281277, 0.00024171313199060517, 0.00024343416810253577, 0.0002443964308851028, 0.0002430264126551558, 0.000244497605837678, 0.0002430324150464162, 0.00024337877914531547, 0.00024655686514954157, 0.0002430324150464162, 0.000244497605837678, 0.0002430324150464162, 0.00024688465054687175, 0.0002443717071944275, 0.0002487058244007445, 0.0002467018605750771, 0.00024448321774221123, 0.00024391991485947907, 0.00024288603606826135, 0.00024457347028002146, 0.00024487900090740386, 0.0002432338264066797, 0.00024508287786734873, 0.0002415072192864792, 0.00024457347028002146, 0.00024508287786734873, 0.00024173289130728003, 0.00024487900090740386, 0.00024457347028002146, 0.0002423715712669157, 0.00024357479074470453, 0.00024423594070488474, 0.00024613200066270484, 0.00024521124848268984, 0.00024354520490477814, 0.00024383812487829913, 0.00024314238511556605, 0.00024168285542734355, 0.00024578864777754643, 0.00024241706454031264, 0.00024508841462000697, 0.00024354520490477814, 0.000246389185098274, 0.0002487425996883957, 0.0002452138067886573, 0.00024146709656987084, 0.00024314642558022253, 0.0002464364332546217, 0.000241517089130931, 0.00024346135030014357, 0.00024294512405145902, 0.00024432347824425833, 0.0002448570794365088, 0.00024277499982680334, 0.00024181491036474222, 0.0002394175401803615, 0.00024776713231480134, 0.0002444292517115431, 0.00024199032551857347, 0.00023795548597495584, 0.0002436723355796402, 0.0002456971728368402, 0.0002443095226647693, 0.00024416913324444133, 0.0002419633144498015, 0.00024431616425617866, 0.0002443095226647693, 0.0002419633144498015, 0.00024805886336119336, 0.00024138850309250446, 0.00024145348163946158, 0.00024189548708820408, 0.00024671258504943365, 0.0002448311613815564, 0.00024813278545475167, 0.0002448599619376855, 0.00024146709656987084, 0.00024397626664772576, 0.00024146709656987084, 0.000241517089130931, 0.00024524443993629154, 0.00024545446817177813, 0.0002435830713025728, 0.00024620214689304346, 0.00024459855563221147, 0.0002408483528255749, 0.0002394526647287167, 0.0002453975894221915, 0.00024455717077813844, 0.0002420284309475297, 0.0002475616615324602, 0.00024313790091847582, 0.0002437131486210327, 0.00024397009434520982, 0.00024292525733083127, 0.00024359055768995446, 0.0002440558638303085, 0.00024062598372281277, 0.0002443964308851028, 0.00024359055768995446, 0.00024062598372281277, 0.00024062598372281277, 0.0002443964308851028, 0.0002443964308851028, 0.00024359055768995446, 0.0002436996992268364, 0.0002443002435119297, 0.00024171313199060517, 0.0002456617027150502, 0.00024359055768995446, 0.00024359055768995446, 0.0002459020833378407, 0.00024861742698975396, 0.00024459660638875223, 0.0002463658945666445, 0.00024162937203095657, 0.00024291289166046253, 0.00024368712891428512, 0.00024151928574481978, 0.00024845278266371804, 0.00024861742698975396, 0.0002462673995273965, 0.0002424332079238282, 0.00024271616004082914, 0.00024463255401929013, 0.00024604602764528296, 0.00024604602764528296, 0.00024359055768995446, 0.0002443964308851028, 0.00024359055768995446, 0.00024663322198005983, 0.00024298117940667064, 0.00024490362145324453, 0.0002470037722210928, 0.00024490362145324453, 0.00024353959422974405, 0.000243869927631914, 0.00024617486240207075, 0.00024219215194524563, 0.00024188770637797771, 0.0002431145557413361, 0.00024219215194524563, 0.00024811001729705014, 0.00024188770637797771, 0.00024617486240207075, 0.0002431145557413361, 0.00023942906519969792, 0.0002444052286739449, 0.0002459216927423711, 0.0002453537729743801, 0.0002459216927423711, 0.00024644381810558463, 0.0002443679750899712, 0.0002437472629334038, 0.00024333957014455458, 0.00024590451783986655, 0.0002444527554876636, 0.00024427209262770224, 0.00024414376563197085, 0.0002443679750899712, 0.00024590451783986655, 0.00024549907528359223, 0.00024569876123660476, 0.0002475731381466191, 0.00024549907528359223, 0.00024569876123660476, 0.0002429202516055215, 0.00024519798804995246, 0.00024831648236700495, 0.00023904930854618282, 0.00024519798804995246, 0.00024327003631178558, 0.0002423494908386071, 0.0002478769539535185, 0.0002483977896944631, 0.0002423494908386071, 0.0002478258928246752, 0.0002403048352177247, 0.0002451554771815026, 0.0002483977896944631, 0.0002455373783215938, 0.0002458562407289385, 0.00024008781021477035, 0.00024624765814121124, 0.00024521124848268984, 0.00024452702088913935, 0.00024247349247874018, 0.0002424517608876542, 0.00024546216833207766, 0.00024094251871492614, 0.0002441964971816284, 0.00024094251871492614, 0.00024340401077525318, 0.00024252673193366772, 0.0002448888143115682, 0.00024260041615560153, 0.000244631309915564, 0.0002448888143115682, 0.0002448888143115682, 0.000244631309915564, 0.000246801715058369, 0.00024487535126835547, 0.00024283863904370299, 0.000246801715058369, 0.00024282790488327043, 0.0002448264904533975, 0.00023960765086204368, 0.0002419459068594555, 0.00024477382233910094, 0.0002427781234295319, 0.0002437658656895555, 0.0002437658656895555, 0.00024729965832977755, 0.00024728390293091997, 0.0002453745916634556, 0.0002454873547664473, 0.0002441261185134163, 0.0002477622322415312, 0.00024148339901899725, 0.00024093186423465864, 0.00024358820874479701, 0.00024548548196802304, 0.000245127500658675, 0.00023862997449582875, 0.00024403627177986274, 0.00024303646679077993, 0.00024234762389250338, 0.00024225715246943932, 0.0002466515592850047, 0.00024327003631178558, 0.00024501724141628067, 0.0002428343527998281, 0.00024265878370108177, 0.00024501724141628067, 0.00024265878370108177, 0.00024757164275069775, 0.0002503641101471956, 0.00024633711311351677, 0.00024398116687664168, 0.00024398116687664168, 0.00024675173420667053, 0.00024382502262296468, 0.00024363171536822683, 0.00024531311944204825, 0.00024533241686641044, 0.0002419166971841183, 0.00024257319754569836, 0.0002445024515514826, 0.0002457986783721674, 0.00023926055108031686, 0.000246101526015623, 0.0002457986783721674, 0.0002483696860542224, 0.00024414225923168086, 0.00024417405907095114, 0.0002449113463656777, 0.00024188656602210036, 0.00024528407141488265, 0.00024344898455756558, 0.00024735431031097297, 0.00024188656602210036, 0.00024528407141488265, 0.0002463180698695266, 0.00024399600314094257, 0.0002482415343339398, 0.0002443300616057039, 0.00024354520490477814, 0.00024472010330645706, 0.00024314238511556605, 0.00024399600314094257, 0.0002430133029521205, 0.00024624456875304944, 0.0002415072192864792, 0.00024288603606826135, 0.00024487900090740386, 0.00024508287786734873, 0.0002415072192864792, 0.0002441334951569486, 0.00024849634685493324, 0.0002441334951569486, 0.00024467286174290276, 0.00024688465054687175, 0.0002444303622246704, 0.0002487058244007445, 0.00024688465054687175, 0.00024409837819401757, 0.00024315602857815212, 0.00024586797331237626, 0.00024508102156376083, 0.0002449432002643171, 0.00024190172413732016, 0.0002441499200551334, 0.00024041602388908027, 0.00024296315913388593, 0.00024372839748169987, 0.0002449432002643171, 0.00024599377686924966, 0.0002430324150464162, 0.00024325004212793382, 0.00024257911874264021, 0.0002467977064698718, 0.0002430324150464162, 0.00024304548863110685, 0.0002449824415214611, 0.00024626010400673795, 0.00024303646679077993, 0.00023862997449582875, 0.00024403627177986274, 0.0002441714718239413, 0.0002444953246064326, 0.0002452259289738141, 0.00024253715609492916, 0.0002443787815978643, 0.0002452259289738141, 0.00024061455315003603, 0.00024045939302542866, 0.00024350167857194226, 0.000248368167644542, 0.00024275878993627853, 0.0002483977896944631, 0.0002410120296058159, 0.0002483977896944631, 0.000242176062995024, 0.00024678110761546125, 0.000242176062995024, 0.0002423494908386071, 0.00024215173793937, 0.0002425363000628551, 0.0002423494908386071, 0.00024127085234501417, 0.00024292476465049165, 0.00024379239342050977, 0.00024352094911342964, 0.0002424332079238282, 0.0002444593830347764, 0.0002435857370071686, 0.00024292476465049165, 0.00024127085234501417, 0.00024556840838963677, 0.00024292476465049165, 0.00024422405066785855, 0.00024084505286366454, 0.00024663155310997685, 0.00024340586957502153, 0.00024084505286366454, 0.0002423722021192633, 0.0002438392750481024, 0.0002452138067886573, 0.00024524443993629154, 0.00024591348118266733, 0.0002480238810691833, 0.0002448599619376855, 0.00024813278545475167, 0.0002452138067886573, 0.00024337205392246648, 0.00024223346526395465, 0.00025062940334567125, 0.00024274323005119385, 0.00024287632473919064, 0.00024503204689104807, 0.00024858071669683717, 0.00024347829015825248, 0.00024260041615560153, 0.00024525604172479914, 0.00024347829015825248, 0.0002448888143115682, 0.0002415177087800803, 0.00024359055768995446, 0.00024288453224786445, 0.0002440558638303085, 0.00024604602764528296, 0.00024604602764528296, 0.0002436996992268364, 0.0002456617027150502, 0.00024298868717567216, 0.0002409793965199141, 0.00024482632394743926, 0.00024414376563197085, 0.0002444527554876636, 0.0002444527554876636, 0.0002421308995913786, 0.00024427209262770224, 0.00024831648236700495, 0.0002415058793584726, 0.0002428343527998281, 0.0002457986783721674, 0.0002457986783721674, 0.00024422094169226845, 0.00023926055108031686, 0.00023926055108031686, 0.00024059421082327464, 0.0002419964252659613, 0.000244655675289404, 0.0002469683913438414, 0.0002441714718239413, 0.00024234762389250338, 0.00024336807337961965, 0.00024549907528359223, 0.0002444693867461774, 0.0002475731381466191, 0.00024380004984840483, 0.00024849209575645285, 0.00024053193287270749, 0.00024380004984840483, 0.00024549907528359223, 0.00024469884154887074, 0.00024331884891595288, 0.00024667141778575874, 0.00024671258504943365, 0.00025002815875525684, 0.00024138850309250446, 0.00024754855566392654, 0.00024171313199060517, 0.0002440558638303085, 0.00024397009434520982, 0.00024359055768995446, 0.0002456617027150502, 0.0002443964308851028, 0.00024359055768995446, 0.00024346630606077348, 0.00024363171536822683, 0.00024397161417673584, 0.00024327003631178558, 0.00024519352265585983, 0.00024236113744469882, 0.0002443717071944275, 0.00024442340952152104, 0.0002441334951569486, 0.0002450348698249165, 0.0002438555654094264, 0.0002391466489724834, 0.00024164564803275075, 0.0002438555654094264, 0.0002450348698249165, 0.00024507893538536305, 0.0002446222911766675, 0.0002462683142192615, 0.00024490362145324453, 0.0002480127923805409, 0.0002462683142192615, 0.00023852308662595265, 0.0002445425997328104, 0.00024490362145324453, 0.00024195890183197963, 0.00024365490655739097, 0.00023852308662595265, 0.0002429202516055215, 0.00024225715246943932, 0.0002429035702453355, 0.0002428343527998281, 0.00024265878370108177, 0.00024831648236700495, 0.00024236113744469882, 0.00024688465054687175, 0.00024249210390317553, 0.0002443717071944275, 0.0002403800255685067, 0.0002394526647287167, 0.0002475616615324602, 0.0002420284309475297, 0.00024313790091847582, 0.00024521666676369106, 0.00024224732039437803, 0.0002454678183428064, 0.0002429202516055215, 0.0002428343527998281, 0.00024831648236700495, 0.00024765341710454906, 0.00025149291061845485, 0.00024402751616408696, 0.00024454904653747563, 0.00024649006502049906, 0.00024443378660188466, 0.00024366880582621706, 0.00024586797331237626, 0.00024296315913388593, 0.00024366880582621706, 0.00024274276956485718, 0.00024041046797782583, 0.0002446843860615644, 0.00023949788871828936, 0.00024765720387978857, 0.00024504692841025044, 0.0002483957887841819, 0.00024438209101169527, 0.0002446843860615644, 0.00024320270961795574, 0.00024525820797733565, 0.00024265083079008963, 0.0002429871339128543, 0.00024677722168488806, 0.0002429871339128543, 0.00024593386414542394]\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for i in range(10):\n",
    "    # doc_weights, doc_vocab = createWordVectors(\"document.txt\", False)\n",
    "    tWeights, cWeights, vocab, sequences = createWordVectors(\"summary.txt\", False)\n",
    "    cWeights = np.array(cWeights).reshape(128, 4096)\n",
    "\n",
    "    #target and context weights for the summary model\n",
    "    vocab = np.array(vocab)\n",
    "    # sequences = np.array(sequences)\n",
    "\n",
    "    #ToDo: generate positive and negative skip-grams for testing v/\n",
    "    #ToDo: cloud version of ngram_freq.csv\n",
    "    #create sequence function?\n",
    "\n",
    "    #use the weights to predict the samples, produce a final evaluation (0-1)\n",
    "    #model is not evaluated correctly\n",
    "    # USE THE DOCUMENT VOCAB TO TRAIN THE SUMMARY WEIGHTS!!!!!!!!!!!!\n",
    "    score = 0\n",
    "    guesses = []\n",
    "    answers = []\n",
    "    probs = []\n",
    "    badSample = 0\n",
    "    goodSamples = []\n",
    "    for k in keySamples:\n",
    "        if  doc_vocab[k[0]] in list(vocab) and doc_vocab[k[1]] in list(vocab):\n",
    "            t_idx = list(vocab).index(doc_vocab[k[0]])\n",
    "            c_idx = list(vocab).index(doc_vocab[k[1]])\n",
    "            goodSamples.append(k)\n",
    "            answers.append(k[2])\n",
    "        \n",
    "    for g in goodSamples:\n",
    "        t_idx = list(vocab).index(doc_vocab[g[0]])\n",
    "        c_idx = list(vocab).index(doc_vocab[g[1]])\n",
    "\n",
    "        Ht = tWeights[t_idx]\n",
    "        HtWO = np.matmul(Ht, cWeights)\n",
    "        softSum = 0\n",
    "        for h in HtWO:\n",
    "            softSum = softSum + math.e ** h\n",
    "        \n",
    "        prob = (math.e ** HtWO[c_idx]) / softSum\n",
    "        guess = 1 if prob > (1 / softSum) else 0\n",
    "        guesses.append(float(guess))\n",
    "        probs.append(prob)\n",
    "\n",
    "        # tM = tWeights[t_idx].reshape(1, 128)\n",
    "        # cM = cWeights[c_idx].reshape(128, 1)\n",
    "\n",
    "        # unSquashed = float(np.matmul(tM, cM))\n",
    "        # guess = 1 / (1 + math.e ** -unSquashed)\n",
    "        # print(guess)\n",
    "        # guess = 1 if guess > 0.5 else 0\n",
    "        # guesses.append(float(guess))\n",
    "        \n",
    "    cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "    m = tf.keras.metrics.Accuracy()\n",
    "\n",
    "    m.update_state(np.array(answers), np.array(guesses))\n",
    "    entroScore = cce(np.array(answers), np.array(guesses)).numpy() \n",
    "    accScore = m.result().numpy()\n",
    "\n",
    "    scores.append(accScore)    \n",
    "    # for k in keySamples:\n",
    "    #     try:\n",
    "    #         t_idx = list(vocab).index(doc_vocab[k[0]])\n",
    "    #         c_idx = list(vocab).index(doc_vocab[k[1]])\n",
    "\n",
    "    #     except:\n",
    "    #         continue\n",
    "\n",
    "    #     tM = tWeights[t_idx].reshape(1, 128)\n",
    "    #     cM = cWeights[c_idx].reshape(128, 1)\n",
    "\n",
    "    #     unSquashed = float(np.matmul(tM, cM))\n",
    "    #     guess = 1 / (1 + math.e ** -unSquashed)\n",
    "    #     guess = 1 if guess > 0.5 else 0\n",
    "    #     guesses.append(guess)\n",
    "        \n",
    "    #     diff = abs(guess - k[2])\n",
    "    #     mini_score = (1 - diff) * (100 / (len(keySamples) - badSample))\n",
    "    #     score = score + mini_score\n",
    "\n",
    "    # scores.append(score)\n",
    "    # print(str(missed) + str(\" / \") + str(len(keyWords)))\n",
    "    # print(\"score: \" + str(round(score, 2)))\n",
    "    # print(guesses)\n",
    "\n",
    "avg = round(np.average(scores), 2)\n",
    "print(\"Average accuracy: \" + str(avg))\n",
    "print(scores)\n",
    "print(probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tWeights, cWeights, vocab, sequences = createWordVectors(\"christmas_carol.txt\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bRep(vec):\n",
    "    vec = np.array(vec)\n",
    "    bVec = []\n",
    "    bVec.append([0] * len(vec[0]))\n",
    "\n",
    "    for v_index, v in enumerate(vec):\n",
    "        if v_index == 0:\n",
    "            continue\n",
    "        bigVec = []\n",
    "        unitVec = []\n",
    "        squaredSum = 0\n",
    "        for vi_index, vi in enumerate(v):\n",
    "            vecNum = vec[v_index][vi_index] - vec[v_index - 1][vi_index]\n",
    "            squaredSum = squaredSum + (vecNum ** 2)\n",
    "            bigVec.append(vecNum)\n",
    "\n",
    "        magnitude = math.sqrt(squaredSum)\n",
    "        for b in bigVec:\n",
    "            magnitude = 1\n",
    "            b = round(b / magnitude, 3)\n",
    "            unitVec.append(b)\n",
    "        \n",
    "        bVec.append(unitVec)\n",
    "\n",
    "    return bVec\n",
    "\n",
    "myArray = [[2, 5, 2, 4], [1, 2, 3, 2]]\n",
    "test = bRep(myArray)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['the', 'large', 'man', 'walked', 'through', 'the', 'brown', 'and', 'sturdy', 'door', 'this', 'afternoon']\n",
    "sum = ['the', 'man', 'walked', 'through', 'the', 'door']\n",
    "corpusVec = []\n",
    "sumVec = []\n",
    "for c in corpus:\n",
    "    cVec = tWeights[vocab.index(c)]\n",
    "    corpusVec.append(cVec)\n",
    "print(np.array(corpusVec).shape)\n",
    "corpusVec = np.array(corpusVec).reshape(12, 3)\n",
    "print('corpus vector')\n",
    "print(corpusVec)\n",
    "\n",
    "for c in sum:\n",
    "    cVec = tWeights[vocab.index(c)]\n",
    "    sumVec.append(cVec)\n",
    "sumVec = np.array(sumVec).reshape(6, 3)\n",
    "print('summary vector')\n",
    "print(sumVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "embed = hub.load(module_url)\n",
    "embeddings = embed([\"The\", \"cat\", \"in\", \"the\", \"hat\"])\n",
    "print(embeddings.shape)  #(3,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing mplot3d toolkits, numpy and matplotlib\n",
    "from mpl_toolkits import mplot3d\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "fig = plt.figure()\n",
    " \n",
    "# syntax for 3-D projection\n",
    "ax = plt.axes(projection ='3d')\n",
    "\n",
    "corpusVec = bRep(np.array(corpusVec).reshape(12,3))\n",
    "print(corpusVec)\n",
    " \n",
    "# defining all 3 axes\n",
    "z = corpusVec[2]\n",
    "x = corpusVec[0]\n",
    "y = corpusVec[1]\n",
    " \n",
    "# plotting\n",
    "ax.scatter(x, y, z, 'green')\n",
    "ax.set_title('corpus vector')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing mplot3d toolkits, numpy and matplotlib\n",
    "from mpl_toolkits import mplot3d\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    " \n",
    "fig = plt.figure()\n",
    " \n",
    "# syntax for 3-D projection\n",
    "ax = Axes3D(fig)\n",
    "sumVec = bRep(np.array(sumVec).reshape(6,3))\n",
    "print(sumVec)\n",
    " \n",
    "# defining all 3 axes\n",
    "z = sumVec[2]\n",
    "x = sumVec[0]\n",
    "y = sumVec[1]\n",
    " \n",
    "# plotting\n",
    "ax.scatter(x, y, z, 'green')\n",
    "ax.set_title('corpus vector')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing mplot3d toolkits, numpy and matplotlib\n",
    "from mpl_toolkits import mplot3d\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "fig = plt.figure()\n",
    " \n",
    "# syntax for 3-D projection\n",
    "ax = plt.axes(projection ='3d')\n",
    "\n",
    "A = [[0, 0, 0], [1, 1, 1]]\n",
    "B = [[1, 1, 0], [0, 0, 1]]\n",
    "A = np.array(A).reshape(3, 2)\n",
    "B = np.array(B).reshape(3, 2)\n",
    "C = A\n",
    "\n",
    "# defining all 3 axes\n",
    "z = C[2]\n",
    "x = C[0]\n",
    "y = C[1]\n",
    "\n",
    "# plotting\n",
    "ax.scatter(x, y, z, 'blue')\n",
    "ax.scatter(B[0], B[1], B[2], 'blue')\n",
    "ax.set_title('corpus vector')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tWeights = [-0.094491, -0.443977, 0.313917, -0.490796, -0.229903, 0.065460, 0.072921, 0.172246, -0.357751, 0.104514, -0.463000, 0.079367, -0.226, -0.1547, -0.0384, 0.4061, -0.1928, -0.4420, 0.1818, 0.0883, 0.2776, -0.0553, 0.4918, 0.2631]\n",
    "cWeights = [0.0230, 0.4799, 0.4321, 0.3758, -0.3647, -0.1198, 0.2661, -0.3510, -0.3680, 0.4248, -0.2571, -0.1488, 0.0339, 0.3538, -0.1449, 0.1309, 0.4224, 0.3645, 0.4679, -0.0203, -0.4239, -0.4388, 0.2686, -0.4468]\n",
    "tWeights = np.array(tWeights)\n",
    "cWeights = np.array(cWeights)\n",
    "tWeights = tWeights.reshape(8, 3)\n",
    "cWeights = cWeights.reshape(3, 8)\n",
    "Ht = tWeights[1]\n",
    "print(Ht)\n",
    "# Ht = np.array(Ht).reshape(8, 3)\n",
    "# cWeights = cWeights.reshape(3, 8)\n",
    "HtWO = np.matmul(Ht, cWeights)\n",
    "print(HtWO)\n",
    "softSum = 0\n",
    "for h in HtWO:\n",
    "    softSum = softSum + math.e ** h\n",
    "print(softSum)\n",
    "print(math.e ** HtWO[1] / softSum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.15.0-py3-none-any.whl (3.4 MB)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Using cached tokenizers-0.10.3-cp38-cp38-win_amd64.whl (2.0 MB)\n",
      "Requirement already satisfied: requests in c:\\users\\brad\\anaconda3\\lib\\site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from transformers) (1.19.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from transformers) (2020.10.15)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from transformers) (4.50.2)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Using cached huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
      "Collecting sacremoses\n",
      "  Using cached sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\brad\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\brad\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (0.17.0)\n",
      "Requirement already satisfied: click in c:\\users\\brad\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in c:\\users\\brad\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Installing collected packages: tokenizers, huggingface-hub, sacremoses, transformers\n",
      "Successfully installed huggingface-hub-0.2.1 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.15.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "huggingface-hub 0.2.1 requires packaging>=20.9, but you'll have packaging 20.4 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing c:\\users\\brad\\appdata\\local\\pip\\cache\\wheels\\52\\19\\88\\6625593382e23a926740e6fcee0f2df0a0de25766094842a28\\sentence_transformers-2.1.0-py3-none-any.whl\n",
      "Requirement already satisfied: nltk in c:\\users\\brad\\anaconda3\\lib\\site-packages (from sentence-transformers) (3.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\brad\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.50.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\brad\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.5.2)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.96-cp38-cp38-win_amd64.whl (1.1 MB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\brad\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.19.2)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\brad\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.2.1)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.11.2-cp38-cp38-win_amd64.whl (985 kB)\n",
      "Collecting torch>=1.6.0\n",
      "  Using cached torch-1.10.1-cp38-cp38-win_amd64.whl (226.6 MB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\brad\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.23.2)\n",
      "Requirement already satisfied: tokenizers>=0.10.3 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.10.3)\n",
      "Requirement already satisfied: regex in c:\\users\\brad\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers) (2020.10.15)\n",
      "Requirement already satisfied: joblib in c:\\users\\brad\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers) (0.17.0)\n",
      "Requirement already satisfied: click in c:\\users\\brad\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers) (7.1.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (5.3.1)\n",
      "Requirement already satisfied: requests in c:\\users\\brad\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.24.0)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\brad\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.46)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (20.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\brad\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from huggingface-hub->sentence-transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from torchvision->sentence-transformers) (8.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (2.1.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2020.6.20)\n",
      "Requirement already satisfied: six in c:\\users\\brad\\anaconda3\\lib\\site-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.15.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\brad\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.4.7)\n",
      "Installing collected packages: sentencepiece, torch, torchvision, sentence-transformers\n",
      "Successfully installed sentence-transformers-2.1.0 sentencepiece-0.1.96 torch-1.10.1 torchvision-0.11.2\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers\n",
    "%pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading:   5%|▌         | 76.5M/1.42G [04:46<1:23:49, 267kB/s]\n",
      "Downloading:   1%|▏         | 21.3M/1.42G [04:08<4:31:54, 85.8kB/s]\n",
      "Downloading:  14%|█▎        | 67.6M/499M [01:03<06:45, 1.06MB/s]\n",
      "Downloading:  14%|█▍        | 70.5M/499M [01:59<12:06, 589kB/s] \n",
      "Downloading:   3%|▎         | 41.0M/1.42G [00:15<09:18, 2.47MB/s]\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1.42G/1.42G [09:02<00:00, 2.62MB/s]\n",
      "Downloading: 100%|██████████| 52.0/52.0 [00:00<00:00, 17.6kB/s]\n",
      "Downloading: 100%|██████████| 239/239 [00:00<00:00, 232kB/s]\n",
      "Downloading: 100%|██████████| 1.36M/1.36M [00:00<00:00, 2.67MB/s]\n",
      "Downloading: 100%|██████████| 1.17k/1.17k [00:00<00:00, 587kB/s]\n",
      "Downloading: 100%|██████████| 798k/798k [00:00<00:00, 1.84MB/s]\n",
      "Downloading: 100%|██████████| 191/191 [00:00<00:00, 95.5kB/s]\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('stsb-roberta-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score: 0.5372869372367859\n"
     ]
    }
   ],
   "source": [
    "sentence1 = \"the large man walked through the brown and sturdy door this afternoon\"\n",
    "sentence2 = \"the man walked through the door\"\n",
    "\n",
    "docTxt = textToArray(\"document.txt\")\n",
    "docTxt = ' '.join(docTxt)\n",
    "sumTxt = textToArray(\"summary.txt\")\n",
    "sumTxt = ' '.join(sumTxt)\n",
    "\n",
    "sentence1 = docTxt\n",
    "sentence2 = sumTxt\n",
    "\n",
    "# encode sentences to get their embeddings\n",
    "embedding1 = model.encode(sentence1, convert_to_tensor=True)\n",
    "embedding2 = model.encode(sentence2, convert_to_tensor=True)\n",
    "\n",
    "# compute similarity scores of two embeddings\n",
    "cosine_scores = util.pytorch_cos_sim(embedding1, embedding2)\n",
    "# print(\"Sentence 1:\", sentence1)\n",
    "# print(\"Sentence 2:\", sentence2)\n",
    "print(\"Similarity score:\", cosine_scores.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4029007060710259\n"
     ]
    }
   ],
   "source": [
    "print(random.random())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c931ba401747e1100110d99c7b2e1195adf3961a7e00160e720e39c4d164b397"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
